{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"Q3RCggiUo26S"},"outputs":[],"source":["from google.colab import drive\n","\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"01rnkm3loUxl"},"outputs":[],"source":["#!unzip \"/content/drive/MyDrive/ELE401/model2_DeepLab/Dataset_V2.zip\" -d \"/content/drive/MyDrive/ELE401/model2_DeepLab/Dataset_V2\"\n"]},{"cell_type":"markdown","metadata":{"id":"706ZJ6E_pIv4"},"source":["## HELPER CLASSES"]},{"cell_type":"markdown","metadata":{"id":"Av8Nmq3_pWYx"},"source":["### METRICS CLASSES"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"--6KaWFlpZo3"},"outputs":[],"source":["import re\n","import torch.nn as nn\n","\n","\n","class BaseObject(nn.Module):\n","    def __init__(self, name=None):\n","        super().__init__()\n","        self._name = name\n","\n","    @property\n","    def __name__(self):\n","        if self._name is None:\n","            name = self.__class__.__name__\n","            s1 = re.sub(\"(.)([A-Z][a-z]+)\", r\"\\1_\\2\", name)\n","            return re.sub(\"([a-z0-9])([A-Z])\", r\"\\1_\\2\", s1).lower()\n","        else:\n","            return self._name\n","\n","\n","class Metric(BaseObject):\n","    pass\n","\n","\n","class Loss(BaseObject):\n","    def __add__(self, other):\n","        if isinstance(other, Loss):\n","            return SumOfLosses(self, other)\n","        else:\n","            raise ValueError(\"Loss should be inherited from `Loss` class\")\n","\n","    def __radd__(self, other):\n","        return self.__add__(other)\n","\n","    def __mul__(self, value):\n","        if isinstance(value, (int, float)):\n","            return MultipliedLoss(self, value)\n","        else:\n","            raise ValueError(\"Loss should be inherited from `BaseLoss` class\")\n","\n","    def __rmul__(self, other):\n","        return self.__mul__(other)\n","\n","\n","class SumOfLosses(Loss):\n","    def __init__(self, l1, l2):\n","        name = \"{} + {}\".format(l1.__name__, l2.__name__)\n","        super().__init__(name=name)\n","        self.l1 = l1\n","        self.l2 = l2\n","\n","    def __call__(self, *inputs):\n","        return self.l1.forward(*inputs) + self.l2.forward(*inputs)\n","\n","\n","class MultipliedLoss(Loss):\n","    def __init__(self, loss, multiplier):\n","\n","        # resolve name\n","        if len(loss.__name__.split(\"+\")) > 1:\n","            name = \"{} * ({})\".format(multiplier, loss.__name__)\n","        else:\n","            name = \"{} * {}\".format(multiplier, loss.__name__)\n","        super().__init__(name=name)\n","        self.loss = loss\n","        self.multiplier = multiplier\n","\n","    def __call__(self, *inputs):\n","        return self.multiplier * self.loss.forward(*inputs)\n","class Activation(nn.Module):\n","\n","    def __init__(self, name, **params):\n","\n","        super().__init__()\n","\n","        if name is None or name == 'identity':\n","            self.activation = nn.Identity(**params)\n","        elif name == 'sigmoid':\n","            self.activation = nn.Sigmoid()\n","        elif name == 'softmax2d':\n","            self.activation = nn.Softmax(dim=1, **params)\n","        elif name == 'softmax':\n","            self.activation = nn.Softmax(**params)\n","        elif name == 'logsoftmax':\n","            self.activation = nn.LogSoftmax(**params)\n","        elif name == 'argmax':\n","            self.activation = ArgMax(**params)\n","        elif name == 'argmax2d':\n","            self.activation = ArgMax(dim=1, **params)\n","        elif callable(name):\n","            self.activation = name(**params)\n","        else:\n","            raise ValueError('Activation should be callable/sigmoid/softmax/logsoftmax/None; got {}'.format(name))\n","\n","    def forward(self, x):\n","        return self.activation(x)\n","\n","import torch\n","\n","\n","def _take_channels(*xs, ignore_channels=None):\n","    if ignore_channels is None:\n","        return xs\n","    else:\n","        channels = [channel for channel in range(xs[0].shape[1]) if channel not in ignore_channels]\n","        xs = [torch.index_select(x, dim=1, index=torch.tensor(channels).to(x.device)) for x in xs]\n","        return xs\n","\n","\n","def _threshold(x, threshold=None):\n","    if threshold is not None:\n","        return (x > threshold).type(x.dtype)\n","    else:\n","        return x\n","\n","\n","def iou(pr, gt, eps=1e-7, threshold=None, ignore_channels=None):\n","    \"\"\"Calculate Intersection over Union between ground truth and prediction\n","    Args:\n","        pr (torch.Tensor): predicted tensor\n","        gt (torch.Tensor):  ground truth tensor\n","        eps (float): epsilon to avoid zero division\n","        threshold: threshold for outputs binarization\n","    Returns:\n","        float: IoU (Jaccard) score\n","    \"\"\"\n","\n","    pr = _threshold(pr, threshold=threshold)\n","    pr, gt = _take_channels(pr, gt, ignore_channels=ignore_channels)\n","\n","    intersection = torch.sum(gt * pr)\n","    union = torch.sum(gt) + torch.sum(pr) - intersection + eps\n","    return (intersection + eps) / union\n","\n","\n","jaccard = iou\n","\n","\n","def f_score(pr, gt, beta=1, eps=1e-7, threshold=None, ignore_channels=None):\n","    \"\"\"Calculate F-score between ground truth and prediction\n","    Args:\n","        pr (torch.Tensor): predicted tensor\n","        gt (torch.Tensor):  ground truth tensor\n","        beta (float): positive constant\n","        eps (float): epsilon to avoid zero division\n","        threshold: threshold for outputs binarization\n","    Returns:\n","        float: F score\n","    \"\"\"\n","\n","    pr = _threshold(pr, threshold=threshold)\n","    pr, gt = _take_channels(pr, gt, ignore_channels=ignore_channels)\n","\n","    tp = torch.sum(gt * pr)\n","    fp = torch.sum(pr) - tp\n","    fn = torch.sum(gt) - tp\n","\n","    score = ((1 + beta ** 2) * tp + eps) / ((1 + beta ** 2) * tp + beta ** 2 * fn + fp + eps)\n","\n","    return score\n","\n","\n","def accuracy(pr, gt, threshold=0.5, ignore_channels=None):\n","    \"\"\"Calculate accuracy score between ground truth and prediction\n","    Args:\n","        pr (torch.Tensor): predicted tensor\n","        gt (torch.Tensor):  ground truth tensor\n","        eps (float): epsilon to avoid zero division\n","        threshold: threshold for outputs binarization\n","    Returns:\n","        float: precision score\n","    \"\"\"\n","    pr = _threshold(pr, threshold=threshold)\n","    pr, gt = _take_channels(pr, gt, ignore_channels=ignore_channels)\n","\n","    tp = torch.sum(gt == pr, dtype=pr.dtype)\n","    score = tp / gt.view(-1).shape[0]\n","    return score\n","\n","\n","def precision(pr, gt, eps=1e-7, threshold=None, ignore_channels=None):\n","    \"\"\"Calculate precision score between ground truth and prediction\n","    Args:\n","        pr (torch.Tensor): predicted tensor\n","        gt (torch.Tensor):  ground truth tensor\n","        eps (float): epsilon to avoid zero division\n","        threshold: threshold for outputs binarization\n","    Returns:\n","        float: precision score\n","    \"\"\"\n","\n","    pr = _threshold(pr, threshold=threshold)\n","    pr, gt = _take_channels(pr, gt, ignore_channels=ignore_channels)\n","\n","    tp = torch.sum(gt * pr)\n","    fp = torch.sum(pr) - tp\n","\n","    score = (tp + eps) / (tp + fp + eps)\n","\n","    return score\n","\n","\n","def recall(pr, gt, eps=1e-7, threshold=None, ignore_channels=None):\n","    \"\"\"Calculate Recall between ground truth and prediction\n","    Args:\n","        pr (torch.Tensor): A list of predicted elements\n","        gt (torch.Tensor):  A list of elements that are to be predicted\n","        eps (float): epsilon to avoid zero division\n","        threshold: threshold for outputs binarization\n","    Returns:\n","        float: recall score\n","    \"\"\"\n","\n","    pr = _threshold(pr, threshold=threshold)\n","    pr, gt = _take_channels(pr, gt, ignore_channels=ignore_channels)\n","\n","    tp = torch.sum(gt * pr)\n","    fn = torch.sum(gt) - tp\n","\n","    score = (tp + eps) / (tp + fn + eps)\n","\n","    return score\n","\n","import torch.nn as nn\n","\n","class JaccardLoss(Loss):\n","    def __init__(self, eps=1.0, activation=None, ignore_channels=None, **kwargs):\n","        super().__init__(**kwargs)\n","        self.eps = eps\n","        self.activation = Activation(activation)\n","        self.ignore_channels = ignore_channels\n","\n","    def forward(self, y_pr, y_gt):\n","        y_pr = self.activation(y_pr)\n","        return 1 - jaccard(\n","            y_pr,\n","            y_gt,\n","            eps=self.eps,\n","            threshold=None,\n","            ignore_channels=self.ignore_channels,\n","        )\n","\n","\n","class DiceLoss(Loss):\n","    def __init__(self, eps=1.0, beta=1.0, activation=None, ignore_channels=None, **kwargs):\n","        super().__init__(**kwargs)\n","        self.eps = eps\n","        self.beta = beta\n","        self.activation = Activation(activation)\n","        self.ignore_channels = ignore_channels\n","\n","    def forward(self, y_pr, y_gt):\n","        y_pr = self.activation(y_pr)\n","        return 1 - f_score(\n","            y_pr,\n","            y_gt,\n","            beta=self.beta,\n","            eps=self.eps,\n","            threshold=None,\n","            ignore_channels=self.ignore_channels,\n","        )\n","\n","\n","class L1Loss(nn.L1Loss, Loss):\n","    pass\n","\n","\n","class MSELoss(nn.MSELoss, Loss):\n","    pass\n","\n","\n","class CrossEntropyLoss(nn.CrossEntropyLoss, Loss):\n","    pass\n","\n","\n","class NLLLoss(nn.NLLLoss, Loss):\n","    pass\n","\n","\n","class BCELoss(nn.BCELoss, Loss):\n","    pass\n","\n","\n","class BCEWithLogitsLoss(nn.BCEWithLogitsLoss, Loss):\n","    pass\n","\n","class IoU(Metric):\n","    __name__ = \"iou_score\"\n","\n","    def __init__(self, eps=1e-7, threshold=0.5, activation=None, ignore_channels=None, **kwargs):\n","        super().__init__(**kwargs)\n","        self.eps = eps\n","        self.threshold = threshold\n","        self.activation = Activation(activation)\n","        self.ignore_channels = ignore_channels\n","\n","    def forward(self, y_pr, y_gt):\n","        y_pr = self.activation(y_pr)\n","        return iou(\n","            y_pr,\n","            y_gt,\n","            eps=self.eps,\n","            threshold=self.threshold,\n","            ignore_channels=self.ignore_channels,\n","        )\n","\n","\n","class Fscore(Metric):\n","    def __init__(self, beta=1, eps=1e-7, threshold=0.5, activation=None, ignore_channels=None, **kwargs):\n","        super().__init__(**kwargs)\n","        self.eps = eps\n","        self.beta = beta\n","        self.threshold = threshold\n","        self.activation = Activation(activation)\n","        self.ignore_channels = ignore_channels\n","\n","    def forward(self, y_pr, y_gt):\n","        y_pr = self.activation(y_pr)\n","        return f_score(\n","            y_pr,\n","            y_gt,\n","            eps=self.eps,\n","            beta=self.beta,\n","            threshold=self.threshold,\n","            ignore_channels=self.ignore_channels,\n","        )\n","\n","\n","class Accuracy(Metric):\n","    def __init__(self, threshold=0.5, activation=None, ignore_channels=None, **kwargs):\n","        super().__init__(**kwargs)\n","        self.threshold = threshold\n","        self.activation = Activation(activation)\n","        self.ignore_channels = ignore_channels\n","\n","    def forward(self, y_pr, y_gt):\n","        y_pr = self.activation(y_pr)\n","        return accuracy(\n","            y_pr,\n","            y_gt,\n","            threshold=self.threshold,\n","            ignore_channels=self.ignore_channels,\n","        )\n","\n","\n","class Recall(Metric):\n","    def __init__(self, eps=1e-7, threshold=0.5, activation=None, ignore_channels=None, **kwargs):\n","        super().__init__(**kwargs)\n","        self.eps = eps\n","        self.threshold = threshold\n","        self.activation = Activation(activation)\n","        self.ignore_channels = ignore_channels\n","\n","    def forward(self, y_pr, y_gt):\n","        y_pr = self.activation(y_pr)\n","        return recall(\n","            y_pr,\n","            y_gt,\n","            eps=self.eps,\n","            threshold=self.threshold,\n","            ignore_channels=self.ignore_channels,\n","        )\n","\n","\n","class Precision(Metric):\n","    def __init__(self, eps=1e-7, threshold=0.5, activation=None, ignore_channels=None, **kwargs):\n","        super().__init__(**kwargs)\n","        self.eps = eps\n","        self.threshold = threshold\n","        self.activation = Activation(activation)\n","        self.ignore_channels = ignore_channels\n","\n","    def forward(self, y_pr, y_gt):\n","        y_pr = self.activation(y_pr)\n","        return precision(\n","            y_pr,\n","            y_gt,\n","            eps=self.eps,\n","            threshold=self.threshold,\n","            ignore_channels=self.ignore_channels,\n","        )"]},{"cell_type":"markdown","metadata":{"id":"NUV8xsO7pcWL"},"source":["### EPOCH CLASS"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wwSfMPhapemS"},"outputs":[],"source":["import numpy as np\n","\n","\n","class Meter(object):\n","    \"\"\"Meters provide a way to keep track of important statistics in an online manner.\n","    This class is abstract, but provides a standard interface for all meters to follow.\n","    \"\"\"\n","\n","    def reset(self):\n","        \"\"\"Reset the meter to default settings.\"\"\"\n","        pass\n","\n","    def add(self, value):\n","        \"\"\"Log a new value to the meter\n","        Args:\n","            value: Next result to include.\n","        \"\"\"\n","        pass\n","\n","    def value(self):\n","        \"\"\"Get the value of the meter in the current state.\"\"\"\n","        pass\n","\n","\n","class AverageValueMeter(Meter):\n","    def __init__(self):\n","        super(AverageValueMeter, self).__init__()\n","        self.reset()\n","        self.val = 0\n","\n","    def add(self, value, n=1):\n","        self.val = value\n","        self.sum += value\n","        self.var += value * value\n","        self.n += n\n","\n","        if self.n == 0:\n","            self.mean, self.std = np.nan, np.nan\n","        elif self.n == 1:\n","            self.mean = 0.0 + self.sum  # This is to force a copy in torch/numpy\n","            self.std = np.inf\n","            self.mean_old = self.mean\n","            self.m_s = 0.0\n","        else:\n","            self.mean = self.mean_old + (value - n * self.mean_old) / float(self.n)\n","            self.m_s += (value - self.mean_old) * (value - self.mean)\n","            self.mean_old = self.mean\n","            self.std = np.sqrt(self.m_s / (self.n - 1.0))\n","\n","    def value(self):\n","        return self.mean, self.std\n","\n","    def reset(self):\n","        self.n = 0\n","        self.sum = 0.0\n","        self.var = 0.0\n","        self.val = 0.0\n","        self.mean = np.nan\n","        self.mean_old = 0.0\n","        self.m_s = 0.0\n","        self.std = np.nan\n","\n","import sys\n","import torch\n","from tqdm import tqdm as tqdm\n","\n","class Epoch:\n","    def __init__(self, model, loss, metrics, stage_name, device=\"cpu\", verbose=True):\n","        self.model = model\n","        self.loss = loss\n","        self.metrics = metrics\n","        self.stage_name = stage_name\n","        self.verbose = verbose\n","        self.device = device\n","\n","        self._to_device()\n","\n","    def _to_device(self):\n","        self.model.to(self.device)\n","        self.loss.to(self.device)\n","        for metric in self.metrics:\n","            metric.to(self.device)\n","\n","    def _format_logs(self, logs):\n","        str_logs = [\"{} - {:.4}\".format(k, v) for k, v in logs.items()]\n","        s = \", \".join(str_logs)\n","        return s\n","\n","    def batch_update(self, x, y):\n","        raise NotImplementedError\n","\n","    def on_epoch_start(self):\n","        pass\n","\n","    def run(self, dataloader):\n","\n","        self.on_epoch_start()\n","\n","        logs = {}\n","        loss_meter = AverageValueMeter()\n","        metrics_meters = {metric.__name__: AverageValueMeter() for metric in self.metrics}\n","\n","        with tqdm(\n","            dataloader,\n","            desc=self.stage_name,\n","            file=sys.stdout,\n","            disable=not (self.verbose),\n","        ) as iterator:\n","            for x, y in iterator:\n","                x, y = x.to(self.device), y.to(self.device)\n","                loss, y_pred = self.batch_update(x, y)\n","\n","                # update loss logs\n","                loss_value = loss.cpu().detach().numpy()\n","                loss_meter.add(loss_value)\n","                loss_logs = {self.loss.__name__: loss_meter.mean}\n","                logs.update(loss_logs)\n","\n","                # update metrics logs\n","                for metric_fn in self.metrics:\n","                    metric_value = metric_fn(y_pred, y).cpu().detach().numpy()\n","                    metrics_meters[metric_fn.__name__].add(metric_value)\n","                metrics_logs = {k: v.mean for k, v in metrics_meters.items()}\n","                logs.update(metrics_logs)\n","\n","                if self.verbose:\n","                    s = self._format_logs(logs)\n","                    iterator.set_postfix_str(s)\n","\n","        return logs\n","\n","\n","class TrainEpoch(Epoch):\n","    def __init__(self, model, loss, metrics, optimizer, device=\"cpu\", verbose=True):\n","        super().__init__(\n","            model=model,\n","            loss=loss,\n","            metrics=metrics,\n","            stage_name=\"train\",\n","            device=device,\n","            verbose=verbose,\n","        )\n","        self.optimizer = optimizer\n","\n","    def on_epoch_start(self):\n","        self.model.train()\n","\n","    def batch_update(self, x, y):\n","\n","        self.optimizer.zero_grad()\n","        prediction = self.model.forward(x)\n","        if isinstance(prediction, dict):\n","          prediction = prediction['out']\n","        loss = self.loss(prediction, y)\n","        loss.backward()\n","        self.optimizer.step()\n","        return loss, prediction\n","\n","\n","class ValidEpoch(Epoch):\n","    def __init__(self, model, loss, metrics, device=\"cpu\", verbose=True):\n","        super().__init__(\n","            model=model,\n","            loss=loss,\n","            metrics=metrics,\n","            stage_name=\"valid\",\n","            device=device,\n","            verbose=verbose,\n","        )\n","\n","    def on_epoch_start(self):\n","        self.model.eval()\n","\n","    def batch_update(self, x, y):\n","\n","        with torch.no_grad():\n","            prediction = self.model.forward(x)\n","            if isinstance(prediction, dict):\n","              prediction = prediction['out']\n","            loss = self.loss(prediction, y)\n","        return loss, prediction"]},{"cell_type":"markdown","metadata":{"id":"E5bjMN0jplDo"},"source":["##DATASET"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Fug-8_GtpnWF"},"outputs":[],"source":["import os\n","#os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n","import numpy as np\n","import cv2\n","import matplotlib.pyplot as plt\n","\n","DATA_DIR = \"/content/drive/MyDrive/ELE401/model2_DeepLab/Dataset_V2\"\n","\n","x_train_dir = os.path.join(DATA_DIR, 'X_train')\n","y_train_dir = os.path.join(DATA_DIR, 'y_train')\n","\n","\n","x_test_dir = os.path.join(DATA_DIR, 'X_test')\n","y_test_dir = os.path.join(DATA_DIR, 'y_test')\n","\n","\n","path, dirs, files = next(os.walk(x_train_dir))\n","file_count = len(files)\n","print(file_count)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"58P-HZeGptI2"},"outputs":[],"source":["CLASSES = ['background' ,'jp_drain', 'drain_bag', 'liquid']\n","height = width = 1024"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Z5V3CVZ_p3b-"},"outputs":[],"source":["from torch.utils.data import Dataset as BaseDataset\n","\n","class Dataset(BaseDataset):\n","      def __init__(\n","        self,\n","        images_dir,\n","        masks_dir,\n","        classes = None,\n","        augmentation = None,\n","        preprocessing = None,\n","    ):\n","        #get images and masks\n","        self.ids_x = sorted(os.listdir(images_dir))\n","        self.ids_y = sorted(os.listdir(masks_dir))\n","\n","\n","        #fullt paths\n","        self.images_fps = [os.path.join(images_dir, image_id) for image_id in self.ids_x]\n","        self.masks_fps = [os.path.join(masks_dir, image_id) for image_id in self.ids_y]\n","\n","        #convert str names to class values on masks\n","        self.class_values = [CLASSES.index(cls.lower()) for cls in classes]\n","        self.augmentation = augmentation\n","        self.preprocessing = preprocessing\n","\n","\n","\n","      def __getitem__(self, i):\n","\n","        #read data\n","        image = cv2.imread(self.images_fps[i])\n","        if image is None:\n","          raise ValueError(f\"Image not found or could not be read: {self.images_fps[i]}\")\n","\n","        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","        #image = cv2.resize(image, (width, height), interpolation=cv2.INTER_LINEAR)\n","\n","\n","\n","        mask = cv2.imread(self.masks_fps[i], cv2.IMREAD_GRAYSCALE)\n","        if mask is None:\n","          raise ValueError(f\"Mask not found or could not be read: {self.masks_fps[i]}\")\n","\n","\n","        image = cv2.resize(image, (width, height), interpolation=cv2.INTER_LINEAR)\n","        mask = cv2.resize(mask, (width, height), interpolation=cv2.INTER_NEAREST)\n","\n","        # extract certain classes\n","        masks = [(mask == v ) for v in self.class_values]\n","        mask = np.stack(masks, axis=-1).astype('float32')\n","\n","\n","        #apply augmentations\n","        if self.augmentation:\n","          sample = self.augmentation(image = image, mask = mask)\n","          image,mask = sample['image'], sample['mask']\n","\n","\n","        #apply preprocessing\n","        if self.preprocessing:\n","          sample = self.preprocessing(image= image, mask= mask)\n","          image, mask = sample['image'], sample['mask']\n","\n","\n","        return self.images_fps[i],image,mask # If filename information needed\n","        #return image,mask\n","\n","\n","      def __len__(self):\n","        return len(self.ids_x)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wI7PrtM_p-uA"},"outputs":[],"source":["def visualize(image, mask, label=None, truth= None):\n","  if truth is None:\n","    plt.figure(figsize=(6,6))\n","    plt.subplot(1,2,1)\n","    plt.imshow(image)\n","    plt.subplot(1,2,2)\n","    plt.imshow(mask)\n","    if label is not None:\n","      plt.title(f\"{label.capitalize()}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"X8RB_ZDkqCTo"},"outputs":[],"source":["  dataset = Dataset(images_dir= x_train_dir, masks_dir= y_train_dir, classes= CLASSES)\n","  #path, image,mask = dataset[301]\n","  image,mask = dataset[350]\n","  print(mask.shape)\n","  print(mask[550,500])\n","  #print(path)\n","  visualize(image= image, mask = mask.squeeze())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QxTLTmLlqD9R"},"outputs":[],"source":["for label in CLASSES:\n","  dataset = Dataset(images_dir= x_train_dir, masks_dir= y_train_dir, classes=[label])\n","  #path,image,mask = dataset[501]\n","  image,mask = dataset[460]\n","  print(mask.shape)\n","  visualize(image= image, mask = mask.squeeze(), label= label)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"o6teRtbVsZj1"},"outputs":[],"source":["import albumentations as albu\n","\n","def to_tensor(x, **kwargs):\n","    return x.transpose(2, 0, 1).astype('float32')\n","\n","\n","\n","def get_preprocessing():\n","    _transform = [\n","        albu.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n","        albu.Lambda(image=to_tensor, mask=to_tensor)\n","    ]\n","    return albu.Compose(_transform)"]},{"cell_type":"markdown","metadata":{"id":"0ILt4LdUtqNy"},"source":["## Track Log Loss Function"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f9cz_9LmsmA3"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","\n","def save_training_logs(epoch, train_logs, valid_logs=None, excel_path=\"loss_history_new.xlsx\", old_iteration = 0):\n","\n","    adjusted_epoch = old_iteration + epoch\n","\n","    data = {\n","        'Epoch': [adjusted_epoch]\n","    }\n","\n","\n","    if 'Cross_Entropy_Loss' in train_logs:\n","        data['Train_Loss'] = [train_logs['Cross_Entropy_Loss']]\n","\n","    # training IoU metrics\n","    for metric in ['IoU_Background', 'IoU_JP', 'IoU_Bag', 'IoU_Liquid']:\n","        if metric in train_logs:\n","            data[f'Train_{metric}'] = [train_logs[metric]]\n","\n","    # validation metrics\n","    if valid_logs:\n","        # validation loss\n","        if 'Cross_Entropy_Loss' in valid_logs:\n","            data['Val_Loss'] = [valid_logs['Cross_Entropy_Loss']]\n","\n","        #validation IoU metrics\n","        for metric in ['Val_IoU_Background', 'Val_IoU_JP', 'Val_IoU_Bag', 'Val_IoU_Liquid']:\n","            if metric in valid_logs:\n","                data[metric] = [valid_logs[metric]]\n","\n","    # DataFrame for current epoch\n","    current_df = pd.DataFrame(data)\n","\n","    # Check if the Excel file exists\n","    if os.path.exists(excel_path):\n","        try:\n","            # Read existing data\n","            existing_df = pd.read_excel(excel_path, engine=\"openpyxl\")\n","\n","            # Check if we need to add columns that might be missing in the existing file\n","            for col in current_df.columns:\n","                if col not in existing_df.columns:\n","                    existing_df[col] = None\n","\n","            # Append new data and save\n","            combined_df = pd.concat([existing_df, current_df], ignore_index=True)\n","            combined_df.to_excel(excel_path, engine=\"openpyxl\", index=False)\n","            print(f\"Successfully saved training logs to: {excel_path}\")\n","        except Exception as e:\n","            # If there's an error, create a backup file\n","            backup_path = f\"{os.path.splitext(excel_path)[0]}_backup.xlsx\"\n","            current_df.to_excel(backup_path, engine=\"openpyxl\", index=False)\n","            print(f\"Error with existing file: {e}\")\n","            print(f\"Saved current epoch data to: {backup_path}\")\n","    else:\n","        # Create a new file if it doesn't exist\n","        current_df.to_excel(excel_path, engine=\"openpyxl\", index=False)"]},{"cell_type":"markdown","metadata":{"id":"YykNCDqyt8Cj"},"source":["##HYPERPARAMETERS"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"t1D2dL8ouL1t"},"outputs":[],"source":["#Batch size\n","BATCH_SIZE = 8\n","DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","NUM_CLASSES = 4\n","\n","#Excel\n","EXCEL_FILE_PATH = '/content/drive/MyDrive/ELE401/model2_DeepLab/excel_logs/loss_history_new.xlsx'\n","\n","#Model\n","STATE_DICT_PATH = '/content/drive/MyDrive/ELE401/model2_DeepLab/state_dicts/modelWeights_00140.pth'\n","MODEL_SAVE_FOLDER = \"/content/drive/MyDrive/ELE401/model2_DeepLab/state_dicts\""]},{"cell_type":"markdown","metadata":{"id":"_GDFXfF4wYxl"},"source":["##DATASETS, DATALOADERS"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EKjc83Bawa3T"},"outputs":[],"source":["from torch.utils.data import DataLoader\n","\n","train_dataset = Dataset(images_dir= x_train_dir, masks_dir= y_train_dir, classes= CLASSES, preprocessing= get_preprocessing())\n","test_dataset = Dataset(images_dir= x_test_dir, masks_dir= y_test_dir, classes= CLASSES, preprocessing= get_preprocessing())\n","\n","\n","\n","train_DataLoader = DataLoader(dataset= train_dataset, batch_size= BATCH_SIZE, shuffle= True)\n","test_DataLoader = DataLoader(dataset= test_dataset, batch_size= 1)\n","\n","#path,image, mask = next(iter(train_DataLoader))\n","image, mask = next(iter(train_DataLoader))\n","#image, mask = next(iter(train_DataLoader))\n","print(f\"Image Path: {path}\")\n","print(f\"Image shape: {image.shape}\")\n","print(f\"Image type: {type(image)}\")\n","print(f\"Image dtype: {image.dtype}\")\n","\n","\n","\n","#print(f\"Mask Shape: {mask.shape}\")\n","print(f\"Mask type: {type(mask)}\")\n","print(f\"Mask shape: {mask.shape}\")\n","print(f\"Mask dtype: {mask.dtype}\")"]},{"cell_type":"markdown","metadata":{"id":"9NO3NZiGwcVo"},"source":["##MODEL, LOSS_FN, OPTIMIZER"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Fb0_6zJHwgl5"},"outputs":[],"source":["#DEVICE AGNOSTIC CODE\n","import torch\n","\n","#HYPER PARAMETERS\n","\n","DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","NUM_CLASSES = 4\n","\n","if DEVICE != \"cuda\":\n","  raise RuntimeError(\"Convert to GPU use model.load_state_dict(torch.load('x.torch', map_location=device))\")\n","\n","\n","\n","from torch import nn #loss_fn\n","from torch import optim #optimizer\n","from torchvision.models import segmentation #segmentation models\n","\n","\n","#MODEL\n","model = segmentation.deeplabv3_resnet50(pretrained=True)\n","model.classifier[4] = nn.Conv2d(256, NUM_CLASSES, kernel_size=(1, 1), stride=(1, 1))\n","model = model.to(DEVICE)  # Move to device before loading\n","\n","# Load state dict\n","loaded_state_dict = torch.load(STATE_DICT_PATH, map_location=DEVICE)\n","model.load_state_dict(loaded_state_dict)\n","\n","\n","#find old iteration\n","import re\n","import os\n","\n","# Get old_iteration from model filename\n","try:\n","    match = re.search(r\"modelWeights_(\\d{5})\\.pth$\", STATE_DICT_PATH)\n","    if not match:\n","        raise ValueError(f\"Could not extract iteration number from model path: {STATE_DICT_PATH}\")\n","\n","    old_iteration = int(match.group(1))\n","    print(f\"Continuing training from iteration: {old_iteration}\")\n","except Exception as e:\n","    print(f\"ERROR: {e}\")\n","    print(\"Provide a model path in the format 'model_XXXXX.pth' where XXXXX is a 5-digit number.\")\n","    raise\n","print(f\"Loaded model state_dict from {STATE_DICT_PATH} (Iteration: {old_iteration})\")\n","\n","#LOSS FUNCTION\n","class_weights = torch.tensor([\n","    1.0,   # Background\n","    20.0,  # JP Drain\n","    20.0,  # Bag\n","    100.0   # Liquid\n","], device=DEVICE)\n","\n","loss_fn = torch.nn.CrossEntropyLoss(weight= class_weights)\n","loss_fn.__name__ = \"Cross_Entropy_Loss\"\n","\n","\n","#OPTIMIZER\n","LEARNING_RATE = 1e-5\n","optimizer = optim.Adam(params= model.parameters(), lr = LEARNING_RATE)\n","\n","scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n","    optimizer,\n","    mode='max',  #IoU maximized\n","    factor=0.5,  # Multiply LR by this factor when reducing\n","    patience=3,  # Number of epochs with no improvement after which LR will be reduced\n","    verbose=True\n",")\n","\n","old_iteration"]},{"cell_type":"markdown","metadata":{"id":"bOkMFQi2wrsK"},"source":["##METRICS"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"J5w2llMLxK8o"},"outputs":[],"source":["#Train metrics\n","metrics_train = [\n","    IoU(threshold=0.5, ignore_channels=[1,2,3]), #0 Background\n","    IoU(threshold=0.5, ignore_channels=[0,2,3]), #1 JP\n","    IoU(threshold=0.5, ignore_channels=[0,1,3]), #2 Bag\n","    IoU(threshold=0.5, ignore_channels=[0,1,2]), #3 Liquid\n","   #Fscore(threshold=0.5),\n","]\n","\n","\n","\n","metrics_train[0].__name__= \"IoU_Background\"\n","metrics_train[1].__name__= \"IoU_JP\"\n","metrics_train[2].__name__= \"IoU_Bag\"\n","metrics_train[3].__name__= \"IoU_Liquid\"\n","\n","\n","\n","#Val metrics\n","metrics_test = [\n","    IoU(threshold=0.5, ignore_channels=[1,2,3]), #0 Background\n","    IoU(threshold=0.5, ignore_channels=[0,2,3]), #1 JP\n","    IoU(threshold=0.5, ignore_channels=[0,1,3]), #2 Bag\n","    IoU(threshold=0.5, ignore_channels=[0,1,2]), #3 Liquid\n","    #Fscore(threshold=0.5),\n","]\n","\n","\n","\n","metrics_test[0].__name__= \"Val_IoU_Background\"\n","metrics_test[1].__name__= \"Val_IoU_JP\"\n","metrics_test[2].__name__= \"Val_IoU_Bag\"\n","metrics_test[3].__name__= \"Val_IoU_Liquid\""]},{"cell_type":"markdown","metadata":{"id":"5arP8XxcxQUc"},"source":["##Train"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"70rNVdBSxSLn"},"outputs":[],"source":["import torch\n","\n","DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","print(\"Using device:\", DEVICE)\n","\n","train_epoch = TrainEpoch(\n","    model,\n","    loss=loss_fn,\n","    metrics=metrics_train,\n","    optimizer=optimizer,\n","    device=DEVICE,\n","    verbose=True,\n",")\n","\n","valid_epoch = ValidEpoch(\n","    model,\n","    loss=loss_fn,\n","    metrics=metrics_test,\n","    device=DEVICE,\n","    verbose=True,\n",")\n","\n","\n","max_score = 0\n","\n","for epoch in range(1, 10000):\n","\n","    print('\\nEpoch: {}'.format(epoch))\n","    train_logs = train_epoch.run(train_DataLoader)\n","    valid_logs = valid_epoch.run(test_DataLoader)\n","\n","    current_iou = train_logs['IoU_Liquid']\n","\n","    scheduler.step(current_iou)\n","\n","    current_lr = optimizer.param_groups[0]['lr']\n","    print(f'Current learning rate: {current_lr:.2e}')\n","\n","    # Save the best model\n","    if max_score < train_logs['IoU_Liquid']:\n","      max_score = train_logs['IoU_Liquid']\n","      best_model_path = os.path.join(MODEL_SAVE_FOLDER, f\"model_best_Weights.pth\")\n","      torch.save(model.state_dict(), best_model_path)\n","      print(f'Best model saved with IOU: {max_score:.4f}')\n","\n","    # Save checkpoint\n","    if epoch % 2 == 0:\n","      checkpoint_number = old_iteration + epoch\n","      checkpoint_path = os.path.join(MODEL_SAVE_FOLDER, f\"modelWeights_{checkpoint_number:05d}.pth\")\n","      torch.save(model.state_dict(), checkpoint_path)\n","      print(f'Checkpoint saved as: {checkpoint_path}')\n","\n","\n","    save_training_logs(epoch=epoch, train_logs=train_logs, valid_logs= valid_logs, excel_path=EXCEL_FILE_PATH, old_iteration= old_iteration)"]},{"cell_type":"markdown","metadata":{"id":"TJgyEe-6-Akn"},"source":["##Testing"]},{"cell_type":"markdown","metadata":{"id":"ag6e22VbA1xG"},"source":["###Last Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wMBEeN3_-CKE"},"outputs":[],"source":["test_epoch = ValidEpoch(\n","    model,\n","    loss=loss_fn,\n","    metrics=metrics_test,\n","    device=DEVICE,\n","    verbose=True,\n",")\n","\n","#MODEL\n","model = segmentation.deeplabv3_resnet50(pretrained=True)\n","model.classifier[4] = nn.Conv2d(256, NUM_CLASSES, kernel_size=(1, 1), stride=(1, 1))\n","model = model.to(DEVICE)  # Move to device before loading\n","\n","# Load state dict\n","loaded_state_dict = torch.load(STATE_DICT_PATH, map_location=DEVICE)\n","model.load_state_dict(loaded_state_dict)\n","\n","\n","test_epoch.run(test_DataLoader)"]},{"cell_type":"markdown","metadata":{"id":"zyB_JGKGA35D"},"source":["###Best Model (Highest Liquid IoU Score)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"F3NYyodA_DhM"},"outputs":[],"source":["#Best model STATE_DICT\n","BEST_STATE_DICT = \"/content/drive/MyDrive/ELE401/model2_DeepLab/state_dicts/modelWeights_00148.pth\"\n","\n","#MODEL\n","model = segmentation.deeplabv3_resnet50(pretrained=True)\n","model.classifier[4] = nn.Conv2d(256, NUM_CLASSES, kernel_size=(1, 1), stride=(1, 1))\n","model = model.to(DEVICE)  # Move to device before loading\n","\n","# Load state dict\n","loaded_state_dict = torch.load(BEST_STATE_DICT, map_location=DEVICE)\n","model.load_state_dict(loaded_state_dict)\n","\n","\n","test_epoch = ValidEpoch(\n","    model,\n","    loss=loss_fn,\n","    metrics=metrics_test,\n","    device=DEVICE,\n","    verbose=True,\n",")\n","\n","\n","test_epoch.run(test_DataLoader)"]},{"cell_type":"markdown","metadata":{"id":"_2Q7I8EuB7Te"},"source":["##RGB Output Demo"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-zk7ygfUUehh"},"outputs":[],"source":["def visualizeData(**images):\n","    \"\"\"PLot images in one row.\"\"\"\n","    n = len(images)\n","    plt.figure(figsize=(8, 5))\n","    for i, (name, image) in enumerate(images.items()):\n","        plt.subplot(1, n, i + 1)\n","        plt.xticks([])\n","        plt.yticks([])\n","        plt.title(' '.join(name.split('_')).title())\n","        plt.imshow(image)\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BRUKpHzsJY0w"},"outputs":[],"source":["DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","\n","#Best model STATE_DICT\n","BEFORE_BEST_STATE_DICT = \"/content/drive/MyDrive/ELE401/model2_DeepLab/state_dicts/modelWeights_00148.pth\"\n","\n","#MODEL\n","trained_model = segmentation.deeplabv3_resnet50(pretrained=True)\n","trained_model.classifier[4] = nn.Conv2d(256, NUM_CLASSES, kernel_size=(1, 1), stride=(1, 1))\n","trained_model = trained_model.to(DEVICE)  # Move to device before loading\n","\n","# Load state dict\n","loaded_state_dict = torch.load(BEFORE_BEST_STATE_DICT, map_location=DEVICE)\n","trained_model.load_state_dict(loaded_state_dict)\n","\n","\n","\n","import torch\n","\n","\n","test_dataset = Dataset(\n","    x_test_dir,\n","    y_test_dir,\n","    augmentation=None,\n","    preprocessing=get_preprocessing(),\n","    classes=CLASSES,\n",")\n","\n","train_dataset = Dataset(\n","    x_train_dir,\n","    y_train_dir,\n","    augmentation=None,\n","    preprocessing=get_preprocessing(),\n","    classes=CLASSES,\n",")\n","\n","\n","# train_dataloader = DataLoader(train_dataset)\n","# test_dataloader = DataLoader(test_dataset)\n","\n","\n","\n","path, image, gt_mask = train_dataset[950]\n","#path, image, gt_mask = test_dataset[108]\n","#image, gt_mask = train_dataset[350]\n","\n","trained_model.eval()\n","with torch.inference_mode():\n","  x_tensor = torch.from_numpy(image).to(DEVICE).unsqueeze(0)\n","  predicted_mask = trained_model(x_tensor)['out']\n","\n","pr_mask = predicted_mask.squeeze().cpu().numpy()\n","\n","#For all labels\n","pr_mask = np.argmax(pr_mask, axis=0)\n","gt_mask = np.argmax(gt_mask, axis=0)\n","\n","#pr_mask = pr_mask[2,:,:]\n","#gt_mask = gt_mask[2,:,:]\n","\n","image_t = image.transpose(1, 2, 0)\n","visualizeData(\n","        image=image_t,\n","        ground_truth_mask=gt_mask,\n","        predicted_mask=pr_mask\n","    )\n","\n","#Convert the predicted mask to numpy and get the predicted class indices\n","predicted_output = torch.argmax(predicted_mask.squeeze(), dim=0).detach().cpu().numpy()\n","Indices = np.unique(predicted_output)\n","print(f\"GT shape: {gt_mask.shape}\")\n","print(f\"PR shape: {pr_mask.shape}\")\n","\n","for i in Indices:\n","  print(CLASSES[i])\n","\n","print(path)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Vvms1h_3J0fE"},"outputs":[],"source":["def overlay_masks(gt_mask, pr_mask):\n","\n","    plt.figure(figsize=(15, 5))\n","\n","    # Display ground truth\n","    plt.subplot(1, 3, 1)\n","    gt_plot = plt.imshow(gt_mask, cmap='viridis')\n","    plt.title(\"Ground Truth\")\n","    plt.axis(\"off\")\n","\n","    # Display prediction\n","    plt.subplot(1, 3, 2)\n","    pr_plot = plt.imshow(pr_mask, cmap='viridis')\n","    plt.title(\"Prediction\")\n","    plt.axis(\"off\")\n","\n","    # Display overlay\n","    plt.subplot(1, 3, 3)\n","\n","    # Create a combined visualization\n","    plt.imshow(gt_mask, cmap='Reds', alpha=0.5)\n","    plt.imshow(pr_mask, cmap='inferno', alpha=0.5)\n","    plt.title(\"Overlay (Blue=GT, Red=Prediction)\")\n","    plt.axis(\"off\")\n","\n","    plt.tight_layout()\n","    plt.show()\n","\n","overlay_masks(gt_mask, pr_mask)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"O82Z-qtuUq6v"},"outputs":[],"source":["def visualize_class_mask(class_mask, num_classes=4):\n","\n","    # Define colors for each class BGR for OpenCV or RGB for matplotlip\n","    colors = [\n","        [0, 0, 0],  # background\n","        [0,  0, 255],  # JP\n","        [255, 255, 0],  #Bag\n","        [0, 255, 0]     # Liq\n","    ]\n","\n","    # Create RGB image\n","    colored_mask = np.zeros((class_mask.shape[0], class_mask.shape[1], 3), dtype=np.uint8)\n","\n","    # Fill in colors based on class indices\n","    for class_idx in range(num_classes):\n","        colored_mask[class_mask == class_idx] = colors[class_idx]\n","\n","    return colored_mask"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rMjKdXnXaJst"},"outputs":[],"source":["rgb_map = visualize_class_mask(predicted_output,4)\n","rgb_map = cv2.cvtColor(rgb_map, cv2.COLOR_RGB2BGR)\n","cv2.imwrite('/content/drive/MyDrive/ELE401/model2_DeepLab/rgb_predicted_map/temp.png', rgb_map)\n","\n","plt.imshow(cv2.imread('/content/drive/MyDrive/ELE401/model2_DeepLab/rgb_predicted_map/temp.png'))"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"JE4Lax9nLcbK"},"outputs":[],"source":["!pip install ultralytics"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YD-gGHHZaUg8"},"outputs":[],"source":["import numpy as np\n","import cv2\n","from matplotlib import pyplot as plt\n","from PIL import Image\n","import math\n","\n","img = Image.open(\"/content/drive/MyDrive/ELE401/model2_DeepLab/rgb_predicted_map/temp.png\")\n","\n","\n","drain = 0\n","blood = 0\n","\n","from ultralytics import YOLO\n","def prepare_for_yolo(image):\n","    #transpose to (H, W, C)\n","    if image.shape[0] == 3 and len(image.shape) == 3:\n","        image = image.transpose(1, 2, 0)\n","\n","    # Reverse normalization\n","    mean = np.array([0.485, 0.456, 0.406]).reshape(-1, 1, 1)\n","    std = np.array([0.229, 0.224, 0.225]).reshape(-1, 1, 1)\n","\n","    if image.shape[-1] == 3:  # If channels last\n","        mean = mean.reshape(1, 1, 3)\n","        std = std.reshape(1, 1, 3)\n","\n","    image = image * std + mean\n","\n","    # Convert to uint8 range [0, 255]\n","    image = (image * 255).clip(0, 255).astype(np.uint8)\n","\n","    return image\n","\n","\n","\n","#path, image, gt_mask = test_dataset[24]\n","print(image.shape)\n","\n","# Prepare image for YOLO\n","image = prepare_for_yolo(image)\n","plt.imshow(image)\n","print(image.shape)\n","\n","model = YOLO(\"/content/drive/MyDrive/ELE401/model2_DeepLab/YOLO_weights/best.pt\")\n","\n","# Run YOLO detection\n","results = model(image, conf= 0.3)\n","boxes = results[0].boxes\n","confidences = boxes.conf.cpu().numpy()\n","best_idx = confidences.argmax()\n","\n","class_id = int(boxes.cls[best_idx].item())\n","confidence = confidences[best_idx]\n","\n","print(f\"Confidence: {confidence}\")\n","CLASS = \"\"\n","\n","# Map class ID to name\n","class_names = results[0].names\n","class_name = class_names[class_id]\n","\n","print(class_names)\n","\n","# Return class type based on detection\n","if \"jp_high\" in class_name.lower():\n","  CLASS = \"JP_high\"\n","\n","if \"jp_low\" in class_name.lower():\n","   CLASS = \"JP_low\"\n","\n","elif \"drain_bag\" in class_name.lower():\n","   CLASS = \"BAG\"\n","\n","if CLASS== \"JP_high\":\n","  mul_value = 350\n","\n","if CLASS== \"JP_low\":\n","  mul_value = 267\n","\n","if CLASS== \"BAG\":\n","  mul_value = 1950  #Not the actual value used, only for demonstration\n","\n","for pixel in img.getdata():\n","    if pixel == (0, 0, 255): # (0, 0, 255) JP Drain, (255,255,0) Drain_Bag\n","        drain += 1\n","    elif pixel == (255, 255, 0): # Bag\n","        drain += 1\n","    elif pixel == (0, 255, 0): # Liquid\n","        blood += 1\n","\n","print(f\"Mul value: {mul_value}\")\n","\n","volume = math.floor(blood / (blood + drain) * mul_value)\n","print(f\"Estimated volume of blood is {volume}ml\")\n","print(blood)"]},{"cell_type":"markdown","metadata":{"id":"l_BoLAcScu2n"},"source":["## Save Overlay Masks"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uH7jC-krlQfP"},"outputs":[],"source":["####  HYPERPARAMS ####\n","BEFORE_BEST_STATE_DICT = \"/content/drive/MyDrive/ELE401/model2_DeepLab/state_dicts/modelWeights_00148.pth\"\n","DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","\n","# classses\n","CLASS_IDX = 3  #1 JP, #2 BAG, #Liq\n","\n","\n","# save directories for overlayed masks\n","JP_overlay_path = \"/content/drive/MyDrive/ELE401/model2_DeepLab/JP_overlayed_masks_V2\"\n","BAG_overlay_dir = \"/content/drive/MyDrive/ELE401/model2_DeepLab/BAG_overlayed_masks_V2\"\n","LIQ_overlay_dir = \"/content/drive/MyDrive/ELE401/model2_DeepLab/Liquid_overlayed_masks_V2\"\n","\n","\n","print(BEFORE_BEST_STATE_DICT)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0gmSmw4MgCmd"},"outputs":[],"source":["import os\n","import cv2\n","import torch\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from torch.utils.data import DataLoader\n","\n","\n","#Best model STATE_DICT\n","\n","\n","#MODEL\n","trained_model = segmentation.deeplabv3_resnet50(pretrained=True)\n","trained_model.classifier[4] = nn.Conv2d(256, NUM_CLASSES, kernel_size=(1, 1), stride=(1, 1))\n","trained_model = trained_model.to(DEVICE)  # Move to device before loading\n","\n","# Load state dict\n","loaded_state_dict = torch.load(BEFORE_BEST_STATE_DICT, map_location=DEVICE)\n","trained_model.load_state_dict(loaded_state_dict)\n","\n","\n","# Initialize  test dataset\n","valid_dataset = Dataset(\n","    x_test_dir,\n","    y_test_dir,\n","    augmentation=None,\n","    preprocessing=get_preprocessing(),\n","    classes=CLASSES,\n",")\n","\n","\n","# Initialize  train dataset\n","train_dataset = Dataset(\n","    x_train_dir,\n","    y_train_dir,\n","    augmentation=None,\n","    preprocessing=get_preprocessing(),\n","    classes=CLASSES,\n",")\n","\n","\n","# Function to overlay masks and save with better contrasting colors\n","def save_overlay(gt_mask, pr_mask, save_path):\n","    plt.figure(figsize=(5, 5))\n","    plt.imshow(gt_mask, cmap=\"Greens\", alpha=0.5, label=\"Ground Truth\")\n","    plt.imshow(pr_mask, cmap=\"inferno\", alpha=0.5, label=\"Predicted Mask\")\n","\n","    plt.savefig(save_path, bbox_inches='tight', pad_inches=0, dpi=300)\n","    plt.close()\n","\n","\n","#CHANGE\n","for idx in range(len(train_dataset)):\n","    #CHANGE\n","    file_path,image, gt_mask = train_dataset[idx]\n","    filename = os.path.splitext(os.path.basename(file_path))[0]\n","\n","    # To process Liquid comment out the lines below.\n","\n","    # if filename.startswith('JP'):\n","    #     overlay_save_dir = JP_overlay_path\n","    #     CLASS_IDX = 1  # JP\n","    # elif filename.startswith('BAG'):\n","    #     overlay_save_dir = BAG_overlay_dir\n","    #     CLASS_IDX = 2  #  BAG\n","    if CLASS_IDX == 3:\n","        overlay_save_dir = LIQ_overlay_dir\n","        CLASS_IDX = 3  # Liquid\n","\n","\n","    x_tensor = torch.from_numpy(image).to(DEVICE).unsqueeze(0)\n","\n","    trained_model.eval()\n","    with torch.inference_mode():\n","        prediction = trained_model(x_tensor)\n","        if isinstance(prediction, dict):\n","          prediction = prediction['out']\n","\n","\n","\n","    pr_mask = prediction.squeeze().cpu().numpy()\n","\n","    #For all labels\n","    pr_mask = np.argmax(pr_mask, axis=0)\n","    gt_mask = np.argmax(gt_mask, axis=0)\n","\n","\n","\n","\n","\n","    # Process ground truth if needed\n","    if gt_mask.ndim > 2:\n","        gt_mask = np.argmax(gt_mask, axis=0)\n","\n","\n","\n","    # Create binary masks\n","    gt_class_mask = (gt_mask == CLASS_IDX).astype(np.float32)\n","    pr_class_mask = (pr_mask == CLASS_IDX).astype(np.float32)\n","\n","    # Overlay save path\n","    overlay_filename = os.path.join(overlay_save_dir, f\"{filename}_overlay.png\")\n","\n","    # Save overlayed image\n","    save_overlay(gt_class_mask, pr_class_mask, overlay_filename)\n","\n","    print(f\"PR mask shape: {pr_mask.shape}, dtype: {pr_mask.dtype}, min: {pr_mask.min()}, max: {pr_mask.max()}\")\n","    print(f\"PR class mask shape: {pr_class_mask.shape}, sum: {pr_class_mask.sum()}\")\n","\n","    print(f\"Saved overlay: {overlay_filename}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Z2GePsFCBoP2"},"outputs":[],"source":["#plt.imshow(gt_class_mask, cmap=\"Greens\", alpha=0.5, label=\"Ground Truth\")  # Blue for ground truth\n","plt.imshow(pr_class_mask, cmap=\"inferno\", alpha=0.5, label=\"Predicted Mask\")  # Red-yellow for prediction\n","np.unique(pr_mask)\n"]},{"cell_type":"markdown","metadata":{"id":"7flpLpWKrK8k"},"source":["##Predicted Masks"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PuSO52IAGs5h"},"outputs":[],"source":["# Define save directory for overlayed masks\n","JP_PRE_dir = \"/content/drive/MyDrive/ELE401/model2_DeepLab/JP_Pre_masks_V2\"\n","BAG_PRE_dir = \"/content/drive/MyDrive/ELE401/model2_DeepLab/BAG_Pre_masks_V2\"\n","\n","# Load model and weights\n","BEST_STATE_DICT = \"/content/drive/MyDrive/ELE401/model2_DeepLab/state_dicts/modelWeights_00148.pth\"\n","print(BEST_STATE_DICT)"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"nFN4WbybGQNk"},"outputs":[],"source":["import os\n","import cv2\n","import torch\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from torch.utils.data import DataLoader\n","\n","\n","# Initialize datasets\n","train_dataset = Dataset(\n","    x_train_dir,\n","    y_train_dir,\n","    augmentation=None,\n","    preprocessing=get_preprocessing(),\n","    classes=CLASSES,\n",")\n","\n","test_dataset = Dataset(\n","    x_test_dir,\n","    y_test_dir,\n","    augmentation=None,\n","    preprocessing=get_preprocessing(),\n","    classes=CLASSES,\n",")\n","\n","\n","import os\n","import cv2\n","import torch\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from torch.utils.data import DataLoader\n","\n","DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","\n","# # Paths for saving predictions\n","# BASE_SAVE_DIR = \"/content/drive/MyDrive/ELE401/predictions\"\n","# JP_SAVE_DIR = os.path.join(BASE_SAVE_DIR, \"JP_predictions\")\n","# BAG_SAVE_DIR = os.path.join(BASE_SAVE_DIR, \"BAG_predictions\")\n","\n","# # Create directories\n","# os.makedirs(JP_SAVE_DIR, exist_ok=True)\n","# os.makedirs(BAG_SAVE_DIR, exist_ok=True)\n","\n","\n","\n","# Initialize model\n","trained_model = segmentation.deeplabv3_resnet50(pretrained=True)\n","trained_model.classifier[4] = nn.Conv2d(256, NUM_CLASSES, kernel_size=(1, 1), stride=(1, 1))\n","trained_model = trained_model.to(DEVICE)\n","\n","# Load state dict\n","loaded_state_dict = torch.load(BEST_STATE_DICT, map_location=DEVICE)\n","trained_model.load_state_dict(loaded_state_dict)\n","trained_model.eval()\n","\n","# Initialize datasets\n","train_dataset = Dataset(\n","    x_train_dir,\n","    y_train_dir,\n","    augmentation=None,\n","    preprocessing=get_preprocessing(),\n","    classes=CLASSES,\n",")\n","\n","train_dataset_org_img = Dataset(\n","    x_train_dir,\n","    y_train_dir,\n","    augmentation=None,\n","    preprocessing= None,\n","    classes=CLASSES,\n",")\n","\n","test_dataset = Dataset(\n","    x_test_dir,\n","    y_test_dir,\n","    augmentation=None,\n","    preprocessing=get_preprocessing(),\n","    classes=CLASSES,\n",")\n","\n","\n","test_dataset_org_img = Dataset(\n","    x_test_dir,\n","    y_test_dir,\n","    augmentation=None,\n","    preprocessing=None,\n","    classes=CLASSES,\n",")\n","\n","\n","def visualizeData(image, ground_truth_mask, predicted_mask, save_path=None):\n","    # Create a figure with 3 subplots\n","    plt.figure(figsize=(15, 5))\n","\n","    # Display original image\n","    plt.subplot(1, 3, 1)\n","    plt.imshow(image)\n","    plt.title('Original Image')\n","    plt.axis('off')\n","\n","    # Display ground truth mask\n","    plt.subplot(1, 3, 2)\n","    plt.imshow(ground_truth_mask)\n","    plt.title('Ground Truth')\n","    plt.axis('off')\n","\n","    # Display predicted mask\n","    plt.subplot(1, 3, 3)\n","    plt.imshow(predicted_mask)\n","    plt.title('Prediction')\n","    plt.axis('off')\n","\n","    # Save or display\n","    if save_path:\n","        plt.savefig(save_path, bbox_inches='tight', dpi=300)\n","        plt.close()\n","    else:\n","        plt.tight_layout()\n","        plt.show()\n","\n","# Function to process a dataset and save predictions\n","def process_dataset(dataset, dataset_name, img_dataset):\n","    print(f\"Processing {dataset_name} dataset...\")\n","\n","    for idx in range(len(dataset)):\n","        try:\n","            # Get data\n","            path, image, gt_mask = dataset[idx]\n","            filename = os.path.splitext(os.path.basename(path))[0]\n","\n","            # Determine save directory\n","            if filename.startswith('JP'):\n","                save_dir = JP_PRE_dir\n","            elif filename.startswith('BAG'):\n","                save_dir = BAG_PRE_dir\n","            else:\n","                continue\n","\n","            # Make prediction\n","            with torch.inference_mode():\n","                x_tensor = torch.from_numpy(image).to(DEVICE).unsqueeze(0)\n","                predicted_mask = trained_model(x_tensor)['out']\n","\n","            # Process the prediction\n","            pr_mask = predicted_mask.squeeze().cpu().numpy()\n","            pr_mask = np.argmax(pr_mask, axis=0)\n","\n","            # Process ground truth\n","            if gt_mask.ndim > 2:\n","                gt_mask = np.argmax(gt_mask, axis=0)\n","\n","            #Get org image\n","            path_org, org_image, org_gt_mask = img_dataset[idx]\n","\n","            # Define save path\n","            save_path = os.path.join(save_dir, f\"{filename}.png\")\n","\n","            # Visualize and save\n","            visualizeData(\n","                image=org_image,\n","                ground_truth_mask=gt_mask,\n","                predicted_mask=pr_mask,\n","                save_path=save_path\n","            )\n","\n","            # Log progress\n","            print(f\"Saved: {save_path}\")\n","\n","            # Get unique class indices in prediction\n","            classes_present = np.unique(pr_mask)\n","            class_names = [CLASSES[i] for i in classes_present]\n","            print(f\"Classes in prediction: {', '.join(class_names)}\")\n","\n","        except Exception as e:\n","            print(f\"Error processing sample {idx}: {e}\")\n","\n","\n","process_dataset(train_dataset, \"training\", train_dataset_org_img)\n","process_dataset(test_dataset, \"test\", test_dataset_org_img)\n","\n","print(\"All predictions saved successfully!\")"]},{"cell_type":"markdown","metadata":{"id":"TS9CwbaDHUKg"},"source":["##EXCEL file"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IKeHWJV2R-Ht"},"outputs":[],"source":["!pip install ultralytics"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Rc02U2xXXAe0"},"outputs":[],"source":["YOLO_PATH = \"/content/drive/MyDrive/ELE401/model2_DeepLab/YOLO_weights/best.pt\"\n","BEST_STATE_DICT = \"/content/drive/MyDrive/ELE401/model2_DeepLab/state_dicts/modelWeights_00148.pth\"\n","\n","\n","print(BEST_STATE_DICT)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"q7VJuokMQoBk"},"outputs":[],"source":["import os\n","import cv2\n","import torch\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import re\n","import math\n","from torch.utils.data import DataLoader\n","\n","\n","def prepare_for_yolo(image):\n","    #Reverse preprocessing to prepare image for YOLO\n","    if image.shape[0] == 3 and len(image.shape) == 3:\n","        image = image.transpose(1, 2, 0)\n","\n","    # Reverse normalization\n","    mean = np.array([0.485, 0.456, 0.406]).reshape(-1, 1, 1)\n","    std = np.array([0.229, 0.224, 0.225]).reshape(-1, 1, 1)\n","\n","    if image.shape[-1] == 3:\n","        mean = mean.reshape(1, 1, 3)\n","        std = std.reshape(1, 1, 3)\n","\n","    image = image * std + mean\n","\n","    image = (image * 255).clip(0, 255).astype(np.uint8)\n","\n","    return image\n","\n","\n","def extract_volume_from_filename(filename):\n","\n","    match = re.search(r'(\\d+)ml', filename)\n","    if match:\n","        return int(match.group(1))\n","    return None\n","\n","\n","def class_wise_iou(pr_mask, gt_mask, num_classes):\n","\n","    iou_scores = {}\n","\n","    # Convert masks to appropriate format if needed\n","    if isinstance(pr_mask, torch.Tensor):\n","        pr_mask = pr_mask.cpu().numpy()\n","    if isinstance(gt_mask, torch.Tensor):\n","        gt_mask = gt_mask.cpu().numpy()\n","\n","    # Handle both one-hot and class index formats\n","    for class_idx in range(num_classes):\n","        if pr_mask.ndim > 2 and pr_mask.shape[0] == num_classes:\n","            # One-hot encoded\n","            pr_bin = pr_mask[class_idx] > 0.5\n","        else:\n","            # Class index map\n","            pr_bin = pr_mask == class_idx\n","\n","        if gt_mask.ndim > 2 and gt_mask.shape[0] == num_classes:\n","            # One-hot encoded\n","            gt_bin = gt_mask[class_idx] > 0.5\n","        else:\n","            # Class index map\n","            gt_bin = gt_mask == class_idx\n","\n","        # Calculate intersection and union\n","        intersection = np.sum(pr_bin & gt_bin)\n","        union = np.sum(pr_bin) + np.sum(gt_bin) - intersection\n","\n","        eps = 1e-7\n","        iou = (intersection + eps) / (union + eps)\n","\n","        iou_scores[f'Class_{class_idx}'] = round(float(iou), 4)\n","\n","    return iou_scores\n","\n","# Device configuration\n","DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","\n","\n","def load_yolo_model(weights_path=YOLO_PATH):\n","    \"\"\"Load YOLOv8 model with specified weights\"\"\"\n","    try:\n","        # Try to import Ultralytics\n","        from ultralytics import YOLO\n","        # Load the model\n","        model = YOLO(weights_path)\n","        return model\n","    except ImportError:\n","        print(\"Ultralytics not found. Installing...\")\n","        os.system(\"pip install -q ultralytics\")\n","        from ultralytics import YOLO\n","        model = YOLO(weights_path)\n","        return model\n","\n","# Load YOLO model\n","yolo_model = load_yolo_model()\n","\n","# Load DeepLabV3 model\n","trained_model = segmentation.deeplabv3_resnet50(pretrained=True)\n","trained_model.classifier[4] = nn.Conv2d(256, NUM_CLASSES, kernel_size=(1, 1), stride=(1, 1))\n","trained_model = trained_model.to(DEVICE)\n","\n","# Load segmentation model weights\n","loaded_state_dict = torch.load(BEST_STATE_DICT, map_location=DEVICE)\n","trained_model.load_state_dict(loaded_state_dict)\n","trained_model.eval()\n","\n","\n","# Initialize datasets\n","train_dataset = Dataset(\n","    x_train_dir,\n","    y_train_dir,\n","    augmentation=None,\n","    preprocessing=get_preprocessing(),\n","    classes=CLASSES,\n",")\n","\n","test_dataset = Dataset(\n","    x_test_dir,\n","    y_test_dir,\n","    augmentation=None,\n","    preprocessing=get_preprocessing(),\n","    classes=CLASSES,\n",")\n","\n","def detect_class_with_yolo(image):\n","\n","    # Prepare image for YOLO\n","    image = prepare_for_yolo(image)\n","\n","    # Run YOLO detection\n","    results = yolo_model(image, conf=0.3)\n","\n","    # Process results\n","    if len(results[0].boxes) == 0:\n","        return None, 0.0  # No detections\n","\n","    # Get the class with highest confidence\n","    boxes = results[0].boxes\n","    confidences = boxes.conf.cpu().numpy()\n","    best_idx = confidences.argmax()\n","\n","    class_id = int(boxes.cls[best_idx].item())\n","    confidence = confidences[best_idx]\n","\n","    # Extract name\n","    class_names = results[0].names\n","    class_name = class_names[class_id]\n","\n","    # Return class type based on detection\n","    if \"jp_high\" in class_name.lower():\n","        return \"JP_high\", confidence\n","    elif \"jp_low\" in class_name.lower():\n","        return \"JP_low\", confidence\n","    elif \"drain_bag\" in class_name.lower():\n","        return \"Drain_Bag\", confidence\n","    else:\n","        return class_name, confidence\n","\n","\n","# Function to visualize masks\n","def visualize_class_mask(class_mask, num_classes=4):\n","\n","    colors = [\n","        [0, 0, 0],      # Background\n","        [0, 0, 255],    # JP\n","        [255, 255, 0],  # Bag\n","        [0, 255, 0]     # Liquid\n","    ]\n","\n","    colored_mask = np.zeros((class_mask.shape[0], class_mask.shape[1], 3), dtype=np.uint8)\n","\n","    for class_idx in range(min(num_classes, len(colors))):\n","        colored_mask[class_mask == class_idx] = colors[class_idx]\n","\n","    return colored_mask\n","\n","\n","\n","def apply_bag_correction(ratio):\n","\n","    a, b, c, d = -0.639004, 1.223345, 0.570415, -0.015097  # E\n","\n","    if 0.4 <= ratio <= 0.62:\n","\n","        corrected_ratio = ratio\n","    else:\n","\n","        corrected_ratio = a*(ratio**3) + b*(ratio**2) + c*ratio + d\n","\n","    # Ensure ratio stays between 0 and 1\n","    corrected_ratio = max(0, min(corrected_ratio, 1))\n","\n","\n","    return corrected_ratio\n","\n","\n","\n","\n","# Function to calculate volume\n","def calculate_volume(mask, class_type, filename):\n","    \"\"\"Calculate volume based on mask and class type\"\"\"\n","\n","    # if class_type is None:\n","    #     # Try to infer from filename\n","    #     if \"JP\" in filename:\n","    #         class_type = \"JP_high\"\n","    #     elif \"BAG\" in filename:\n","    #         class_type = \"Drain_Bag\"\n","    #     else:\n","    #         return 0  # Can't determine class type\n","\n","    # Count pixels for each class\n","    jp_pixels = np.sum(mask == 1)      # JP drain\n","    bag_pixels = np.sum(mask == 2)     # Bag\n","    liquid_pixels = np.sum(mask == 3)  # Liquid )\n","\n","    # Determine which container is relevant based on class_type\n","    if \"jp_low\" in class_type.lower() or \"jp_high\" in class_type.lower() or \"jp\" in class_type.lower():\n","        container_pixels = jp_pixels\n","    elif \"bag\" in class_type.lower() or \"drain_bag\" in class_type.lower():\n","        container_pixels = bag_pixels\n","    else:\n","        return 0  # Unknown container type\n","\n","    # Skip if no liquid or container detected\n","    if liquid_pixels == 0 or container_pixels == 0:\n","        return 0\n","\n","    # Calculate the ratio of liquid to total volume\n","    ratio = liquid_pixels / (liquid_pixels + container_pixels)\n","\n","    # Determine capacity based on class_type\n","    if class_type == \"JP_high\" or class_type == \"JP\":\n","        capacity = 350\n","        volume_ml = math.floor(ratio * capacity)\n","    elif class_type == \"JP_low\":\n","        capacity = 267\n","        volume_ml = math.floor(ratio * capacity)\n","    elif class_type == \"Drain_Bag\" or class_type == \"BAG\":\n","        capacity = 1950\n","        corrected_ratio = apply_bag_correction(ratio)\n","        volume_ml = math.floor(corrected_ratio * capacity)\n","\n","\n","\n","    return volume_ml\n","\n","# Lists to store results\n","results = []\n","undetected_images = []\n","\n","# Process datasets\n","datasets = {\n","    \"Train\": train_dataset,\n","    \"Test\": test_dataset\n","}\n","\n","for dataset_name, dataset in datasets.items():\n","    print(f\"Processing {dataset_name} dataset...\")\n","\n","    for idx in range(len(dataset)):\n","        try:\n","            file_path, image, gt_mask = dataset[idx]\n","            filename = os.path.splitext(os.path.basename(file_path))[0]\n","\n","            print(f\"Processing {filename} ({idx+1}/{len(dataset)})\")\n","\n","            # Get ground truth volume if available\n","            gt_volume = extract_volume_from_filename(filename)\n","\n","            # Detect class type using YOLO model\n","            yolo_class_type, confidence = detect_class_with_yolo(image)\n","\n","            # Check if YOLO detection failed\n","            yolo_detection_failed = False\n","            if yolo_class_type is None:\n","                print(f\"  YOLO detection failed for {filename}, trying filename...\")\n","                yolo_detection_failed = True\n","\n","                # Record the undetected image\n","                undetected_images.append({\n","                    \"Dataset\": dataset_name,\n","                    \"Filename\": filename,\n","                    \"Path\": file_path\n","                })\n","\n","                # Try to determine class from filename\n","                if \"JP\" in filename:\n","                    yolo_class_type = \"JP_high\"\n","                    confidence = 0.0\n","                elif \"BAG\" in filename:\n","                    yolo_class_type = \"Drain_Bag\"\n","                    confidence = 0.0\n","                else:\n","                    # Use default\n","                    yolo_class_type = \"Unknown\"\n","                    confidence = 0.0\n","\n","            # Predict segmentation mask\n","\n","\n","            x_tensor = torch.from_numpy(image).to(DEVICE).unsqueeze(0)\n","\n","            with torch.no_grad():\n","                prediction = trained_model(x_tensor)\n","                if isinstance(prediction, dict):\n","                    prediction = prediction['out']\n","\n","            # Convert to class index map\n","            pr_mask = torch.argmax(prediction, dim=1).squeeze().cpu().numpy()\n","\n","            # Process ground truth\n","            if gt_mask.ndim > 2:\n","                gt_mask = np.argmax(gt_mask, axis=0)\n","\n","            # Calculate IoU scores\n","            iou_scores = class_wise_iou(pr_mask, gt_mask, len(CLASSES))\n","\n","            # Calculate volume using the class type\n","            pred_volume = calculate_volume(pr_mask, yolo_class_type, filename)\n","\n","\n","            # Store results\n","            result = {\n","                \"Dataset\": dataset_name,\n","                \"Filename\": filename,\n","                \"YOLO_Detected\": not yolo_detection_failed,\n","                \"YOLO_Class\": yolo_class_type,\n","                \"YOLO_Confidence\": round(float(confidence), 4) if confidence else 0.0,\n","                \"IoU_Background\": iou_scores.get(\"Class_0\", 0),\n","                \"IoU_JP\": iou_scores.get(\"Class_1\", 0),\n","                \"IoU_Bag\": iou_scores.get(\"Class_2\", 0),\n","                \"IoU_Liquid\": iou_scores.get(\"Class_3\", 0),\n","                \"GT_Volume\": gt_volume,\n","                \"Predicted_Volume\": pred_volume,\n","            }\n","\n","            # Add volume error if ground truth is available\n","            if gt_volume and gt_volume > 0:\n","                result[\"Volume_Error_ml\"] = pred_volume - gt_volume\n","                result[\"Volume_Error_Percent\"] = round((pred_volume - gt_volume) / gt_volume * 100, 2)\n","\n","            results.append(result)\n","            print(f\"  Processed: Class={yolo_class_type}, Volume={pred_volume}ml\")\n","\n","        except Exception as e:\n","            print(f\"Error processing {filename}: {e}\")\n","            import traceback\n","            traceback.print_exc()\n","\n","# Create directories if they don't exist\n","os.makedirs(\"/content/drive/MyDrive/ELE401/model2_DeepLab/excel_results_V3\", exist_ok=True)\n","\n","# Create DataFrame and save to Excel\n","df_results = pd.DataFrame(results)\n","excel_path = \"/content/drive/MyDrive/ELE401/model2_DeepLab/excel_results_V3/yolo_integration_results_V3.xlsx\"\n","df_results.to_excel(excel_path, index=False)\n","\n","print(f\"Results saved to: {excel_path}\")\n","\n","# Save undetected images to a separate Excel file\n","if undetected_images:\n","    df_undetected = pd.DataFrame(undetected_images)\n","    undetected_path = \"/content/drive/MyDrive/ELE401/model2_DeepLab/excel_results_V3/undetected_images_V3.xlsx\"\n","    df_undetected.to_excel(undetected_path, index=False)\n","    print(f\"Undetected images saved to: {undetected_path}\")\n","    print(f\"Total undetected images: {len(undetected_images)}\")\n","else:\n","    print(\"All images were successfully detected by YOLO.\")\n","\n","# Print summary statistics\n","print(\"\\nSummary of Results:\")\n","print(f\"Total processed images: {len(results)}\")\n","print(f\"YOLO detection rate: {sum(df_results['YOLO_Detected']) / len(df_results) * 100:.2f}%\")\n","\n","# Print IoU statistics\n","print(f\"Average IoU_JP: {df_results['IoU_JP'].mean():.4f}\")\n","print(f\"Average IoU_Bag: {df_results['IoU_Bag'].mean():.4f}\")\n","print(f\"Average IoU_Liquid: {df_results['IoU_Liquid'].mean():.4f}\")\n","\n","# Print volume error statistics\n","if 'Volume_Error_Percent' in df_results.columns:\n","    valid_errors = df_results['Volume_Error_Percent'].dropna()\n","    if not valid_errors.empty:\n","        print(f\"Average Volume Error: {valid_errors.mean():.2f}%\")\n","        print(f\"Median Volume Error: {valid_errors.median():.2f}%\")\n","        print(f\"Volume Error Range: {valid_errors.min():.2f}% to {valid_errors.max():.2f}%\")"]},{"cell_type":"code","source":["import pandas as pd\n","\n","# Path to your existing Excel file\n","excel_path = \"/content/drive/MyDrive/ELE401/model2_DeepLab/excel_results_V3/yolo_integration_results_V3.xlsx\"\n","\n","# Read the Excel file\n","df = pd.read_excel(excel_path)\n","\n","print(df['YOLO_Detected'].value_counts())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bUv5mRTnWcg9","executionInfo":{"status":"ok","timestamp":1751447085840,"user_tz":-180,"elapsed":276,"user":{"displayName":"Ali elikkaya","userId":"08456474815891590995"}},"outputId":"6d51ff6e-a1c6-4147-e027-8bc257b33edc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["YOLO_Detected\n","True    1086\n","Name: count, dtype: int64\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xlLRWeManOra"},"outputs":[],"source":["import pandas as pd\n","\n","# Path to your existing Excel file\n","excel_path = \"/content/drive/MyDrive/ELE401/model2_DeepLab/excel_results_V3/yolo_integration_results_V3.xlsx\"\n","\n","# Read the Excel file\n","df = pd.read_excel(excel_path)\n","\n","# Remove unwanted columns if they exist\n","columns_to_remove = [\"YOLO_Detected\"]\n","existing_columns = [col for col in columns_to_remove if col in df.columns]\n","\n","if existing_columns:\n","    df = df.drop(columns=existing_columns)\n","    print(f\"Removed columns: {', '.join(existing_columns)}\")\n","else:\n","    print(\"No matching columns found to remove\")\n","\n","# Sort by the 'Filename' column\n","if \"Filename\" in df.columns:\n","    df = df.sort_values(by=\"Filename\")\n","    print(\"DataFrame sorted by 'Filename'\")\n","else:\n","    print(\"'Filename' column not found in the DataFrame. Skipping sorting.\")\n","\n","# Save the modified DataFrame back to the Excel file\n","df.to_excel(excel_path, index=False)\n","print(f\"Modified Excel file saved to: {excel_path}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"APFLD2n8zhMb"},"outputs":[],"source":["import pandas as pd\n","import os\n","\n","# Path to your existing Excel file\n","excel_path = \"/content/drive/MyDrive/ELE401/model2_DeepLab/excel_results_V3/yolo_integration_results_V3.xlsx\"\n","\n","# Output paths for the separated Excel files\n","bag_excel_path = \"/content/drive/MyDrive/ELE401/model2_DeepLab/excel_results_V3/BAG_results_sorted_V3.xlsx\"\n","jp_excel_path = \"/content/drive/MyDrive/ELE401/model2_DeepLab/excel_results_V3/JP_results_sorted_V3.xlsx\"\n","other_excel_path = \"/content/drive/MyDrive/ELE401/model2_DeepLab/excel_results_V3/other_results_V3.xlsx\"\n","\n","# Read the Excel file\n","df = pd.read_excel(excel_path)\n","print(f\"Read {len(df)} rows from original Excel file\")\n","\n","# Create masks for BAG and JP entries\n","bag_mask = df['Filename'].str.startswith('BAG')\n","jp_mask = df['Filename'].str.startswith('JP')\n","\n","# Split the dataframe\n","bag_df = df[bag_mask].copy()\n","jp_df = df[jp_mask].copy()\n","other_df = df[~(bag_mask | jp_mask)].copy()\n","\n","# Save to separate Excel files\n","bag_df.to_excel(bag_excel_path, index=False)\n","print(f\"Saved {len(bag_df)} BAG entries to: {bag_excel_path}\")\n","\n","jp_df.to_excel(jp_excel_path, index=False)\n","print(f\"Saved {len(jp_df)} JP entries to: {jp_excel_path}\")\n","\n","# If there are any other entries not starting with BAG or JP\n","if len(other_df) > 0:\n","    other_df.to_excel(other_excel_path, index=False)\n","    print(f\"Saved {len(other_df)} other entries to: {other_excel_path}\")\n","else:\n","    print(\"No other entries found\")\n","\n","print(f\"Total entries processed: {len(bag_df) + len(jp_df) + len(other_df)}\")\n","print(f\"Original file had: {len(df)} entries\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DKdALs630JZn"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","\n","# Path to the JP Excel file\n","jp_excel_path = \"/content/drive/MyDrive/ELE401/model2_DeepLab/excel_results_V3/JP_results_sorted_V3.xlsx\"\n","\n","# Load the JP data\n","jp_df = pd.read_excel(jp_excel_path)\n","print(f\"Loaded {len(jp_df)} JP samples from Excel file\")\n","\n","# Check if Volume_Error_Percent column exists\n","if 'Volume_Error_Percent' in jp_df.columns:\n","    # Calculate mean percentage error (ignoring NaN values)\n","    mean_error = jp_df['Volume_Error_Percent'].mean()\n","    median_error = jp_df['Volume_Error_Percent'].median()\n","    std_error = jp_df['Volume_Error_Percent'].std()\n","\n","    # Calculate absolute error statistics\n","    abs_errors = jp_df['Volume_Error_Percent'].abs()\n","    mean_abs_error = abs_errors.mean()\n","    median_abs_error = abs_errors.median()\n","\n","    print(f\"Mean percentage error: {mean_error:.2f}%\")\n","    print(f\"Median percentage error: {median_error:.2f}%\")\n","    print(f\"Standard deviation: {std_error:.2f}%\")\n","    print(f\"Mean absolute percentage error: {mean_abs_error:.2f}%\")\n","    print(f\"Median absolute percentage error: {median_abs_error:.2f}%\")\n","\n","    # Count samples under various threshold values\n","    thresholds = [5, 10, 15, 20, 25, 30]\n","\n","    print(\"\\nSamples under error thresholds:\")\n","    for th in thresholds:\n","        count = (abs_errors <= th).sum()\n","        percentage = (count / len(jp_df)) * 100\n","        print(f\"Error <= {th}%: {count} samples ({percentage:.2f}% of total)\")\n","\n","    # Distribution of errors\n","    print(\"\\nError distribution:\")\n","    bins = [-100, -50, -25, -10, -5, 0, 5, 10, 25, 50, 100, float('inf')]\n","    labels = [\n","        \"< -50%\", \"-50% to -25%\", \"-25% to -10%\", \"-10% to -5%\", \"-5% to 0%\",\n","        \"0% to 5%\", \"5% to 10%\", \"10% to 25%\", \"25% to 50%\", \"50% to 100%\", \"> 100%\"\n","    ]\n","\n","    # Count samples in each bin\n","    error_distribution = pd.cut(jp_df['Volume_Error_Percent'], bins=bins, labels=labels)\n","    error_counts = error_distribution.value_counts().sort_index()\n","\n","    for category, count in error_counts.items():\n","        percentage = (count / len(jp_df)) * 100\n","        print(f\"{category}: {count} samples ({percentage:.2f}% of total)\")\n","\n","else:\n","    print(\"Volume_Error_Percent column not found in the JP Excel file\")\n","\n","\n","#print(f\"IoU Liquid:{np.mean(jp_df[jp_df['IoU_Liquid'] > 0]['IoU_Liquid'])} \")\n","#print(f\"IoU Drain: {np.mean(jp_df[jp_df['IoU_JP'] > 0]['IoU_JP'])}\")\n","\n","print(f\"IoU Liquid:{np.mean(jp_df['IoU_Liquid'])} \")\n","print(f\"IoU Drain: {np.mean(jp_df['IoU_JP'])}\")\n","\n","jp_df_liq_iou_0 = jp_df[jp_df['IoU_Liquid'] == 0]\n","jp_df_jp_iou_0 = jp_df[jp_df['IoU_JP'] == 0]\n","\n","print(f\"LiqIoU=0 : {len(jp_df_liq_iou_0)} \")\n","print(f\"JPIoU=0 : {len(jp_df_jp_iou_0)} \")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sB7ubdLV0eRy"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","\n","# Path to the BAG Excel file\n","bag_excel_path = \"/content/drive/MyDrive/ELE401/model2_DeepLab/excel_results_V3/BAG_results_sorted_V3.xlsx\"\n","\n","# Load the BAG datas\n","bag_df = pd.read_excel(bag_excel_path)\n","print(f\"Loaded {len(bag_df)} BAG samples from Excel file\")\n","\n","# Check if Volume_Error_Percent column exists\n","if 'Volume_Error_Percent' in bag_df.columns:\n","    # Calculate mean percentage error (ignoring NaN values)\n","    mean_error = bag_df['Volume_Error_Percent'].mean()\n","    median_error = bag_df['Volume_Error_Percent'].median()\n","    std_error = bag_df['Volume_Error_Percent'].std()\n","\n","    # Calculate absolute error statistics\n","    abs_errors = bag_df['Volume_Error_Percent'].abs()\n","    mean_abs_error = abs_errors.mean()\n","    median_abs_error = abs_errors.median()\n","\n","    print(f\"Mean percentage error: {mean_error:.2f}%\")\n","    print(f\"Median percentage error: {median_error:.2f}%\")\n","    print(f\"Standard deviation: {std_error:.2f}%\")\n","    print(f\"Mean absolute percentage error: {mean_abs_error:.2f}%\")\n","    print(f\"Median absolute percentage error: {median_abs_error:.2f}%\")\n","\n","    # Count samples under various threshold values\n","    thresholds = [5, 10, 15, 20, 25, 30]\n","\n","    print(\"\\nSamples under error thresholds:\")\n","    for th in thresholds:\n","        count = (abs_errors <= th).sum()\n","        percentage = (count / len(bag_df)) * 100\n","        print(f\"Error <= {th}%: {count} samples ({percentage:.2f}% of total)\")\n","\n","    # Distribution of errors\n","    print(\"\\nError distribution:\")\n","    bins = [-100, -50, -25, -10, -5, 0, 5, 10, 25, 50, 100, float('inf')]\n","    labels = [\n","        \"< -50%\", \"-50% to -25%\", \"-25% to -10%\", \"-10% to -5%\", \"-5% to 0%\",\n","        \"0% to 5%\", \"5% to 10%\", \"10% to 25%\", \"25% to 50%\", \"50% to 100%\", \"> 100%\"\n","    ]\n","\n","    # Count samples in each bin\n","    error_distribution = pd.cut(bag_df['Volume_Error_Percent'], bins=bins, labels=labels)\n","    error_counts = error_distribution.value_counts().sort_index()\n","\n","    for category, count in error_counts.items():\n","        percentage = (count / len(bag_df)) * 100\n","        print(f\"{category}: {count} samples ({percentage:.2f}% of total)\")\n","\n","else:\n","    print(\"Volume_Error_Percent column not found in the BAG Excel file\")\n","\n","print(f\"IoU Liquid:  {bag_df['IoU_Liquid'].mean()}\")\n","print(f\"IoU Bag:  {np.mean(bag_df['IoU_Bag']):.4f}\")\n","\n","\n","\n","bag_df_liq_iou_0 = bag_df[bag_df['IoU_Liquid'] == 0]\n","bag_df_bag_iou_0 = bag_df[bag_df['IoU_JP'] == 0]\n","\n","print(f\"LiqIoU=0 : {len(bag_df_liq_iou_0)} \")\n","print(f\"BAGIoU=0 : {len(bag_df_bag_iou_0)} \")"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["706ZJ6E_pIv4","Av8Nmq3_pWYx","NUV8xsO7pcWL","E5bjMN0jplDo","0ILt4LdUtqNy","YykNCDqyt8Cj","_GDFXfF4wYxl","9NO3NZiGwcVo","bOkMFQi2wrsK","5arP8XxcxQUc","TJgyEe-6-Akn","ag6e22VbA1xG","zyB_JGKGA35D","_2Q7I8EuB7Te","l_BoLAcScu2n","7flpLpWKrK8k"],"gpuType":"A100","machine_shape":"hm","provenance":[],"authorship_tag":"ABX9TyOC7ONyntxYkwWUu4djtLX3"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}